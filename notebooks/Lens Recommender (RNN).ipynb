{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "31c7ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e08b062",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"../raw_data/data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1892c78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering down to essential features & transform datetime feature\n",
    "\n",
    "df = data[['Account: Account ID', 'Product Category (6D)', 'Model  Name', 'Purchase  Date', 'Purchase Month', 'Purchase Year']]\n",
    "df['Purchase  Date'] = pd.to_datetime(df['Purchase  Date'])\n",
    "df = df[df['Purchase  Date']>'2017-04-01']\n",
    "df['Today'] = datetime.today()\n",
    "df['Recency'] = round((df['Today'] - df['Purchase  Date']).dt.days/30,0)\n",
    "df['Recency'] = df['Recency'].astype('int')\n",
    "df.drop(columns=['Purchase  Date', 'Purchase Month', 'Purchase Year', 'Today'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5a7127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering down to customers with purchase history between 3~15\n",
    "df_count = df.groupby('Account: Account ID').count()[['Model  Name']]\n",
    "df_count = df_count[df_count['Model  Name']>=3]\n",
    "df_count = df_count[df_count['Model  Name']<=15]\n",
    "customer_id = df_count.index\n",
    "input_data = df[df['Account: Account ID'].isin(customer_id)]\n",
    "input_data = input_data.rename(columns={'Account: Account ID':'account_id', 'Model  Name':'model', 'Recency':'recency'})[['account_id','model', 'recency']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "998d048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelEncoder model into integers\n",
    "encoder = LabelEncoder()\n",
    "input_data['model'] = encoder.fit_transform(input_data['model'])\n",
    "input_data['model'] = input_data['model'].apply(lambda x: x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ee29f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby customer, aggregate model and recency into arrays\n",
    "input_data = input_data.groupby('account_id').agg(list)\n",
    "input_data['model'] = input_data['model'].apply(lambda x: np.array(x))\n",
    "input_data['recency'] = input_data['recency'].apply(lambda x: np.array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc7bb92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating training_sequence & target_sequence\n",
    "input_data['training_sequence'] = input_data['model'].apply(lambda x: x[:-1])\n",
    "input_data['target_sequence'] = input_data['model'].apply(lambda x: x[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ddd02c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>recency</th>\n",
       "      <th>training_sequence</th>\n",
       "      <th>target_sequence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>account_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1900000215889</th>\n",
       "      <td>[131, 30, 107]</td>\n",
       "      <td>[50, 50, 48]</td>\n",
       "      <td>[131, 30]</td>\n",
       "      <td>[30, 107]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0010o00002AGLLq</th>\n",
       "      <td>[47, 121, 85]</td>\n",
       "      <td>[41, 40, 37]</td>\n",
       "      <td>[47, 121]</td>\n",
       "      <td>[121, 85]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0010o00002AGU1E</th>\n",
       "      <td>[116, 41, 98]</td>\n",
       "      <td>[40, 40, 4]</td>\n",
       "      <td>[116, 41]</td>\n",
       "      <td>[41, 98]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0010o00002AGUKz</th>\n",
       "      <td>[116, 41, 98]</td>\n",
       "      <td>[41, 41, 41]</td>\n",
       "      <td>[116, 41]</td>\n",
       "      <td>[41, 98]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0010o00002AGYLD</th>\n",
       "      <td>[41, 53, 116, 116]</td>\n",
       "      <td>[40, 18, 18, 10]</td>\n",
       "      <td>[41, 53, 116]</td>\n",
       "      <td>[53, 116, 116]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0019000002ACnHZ</th>\n",
       "      <td>[106, 99, 25]</td>\n",
       "      <td>[47, 41, 16]</td>\n",
       "      <td>[106, 99]</td>\n",
       "      <td>[99, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0019000002ACnby</th>\n",
       "      <td>[12, 107, 131]</td>\n",
       "      <td>[41, 41, 36]</td>\n",
       "      <td>[12, 107]</td>\n",
       "      <td>[107, 131]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0019000002ACsYC</th>\n",
       "      <td>[41, 53, 116, 41, 41, 41, 41]</td>\n",
       "      <td>[41, 16, 11, 1, 1, 1, 1]</td>\n",
       "      <td>[41, 53, 116, 41, 41, 41]</td>\n",
       "      <td>[53, 116, 41, 41, 41, 41]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0019000002B5m8N</th>\n",
       "      <td>[41, 41, 97, 84, 54, 91, 54, 99, 3, 3]</td>\n",
       "      <td>[47, 46, 46, 36, 36, 32, 32, 28, 13, 11]</td>\n",
       "      <td>[41, 41, 97, 84, 54, 91, 54, 99, 3]</td>\n",
       "      <td>[41, 97, 84, 54, 91, 54, 99, 3, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0019000002B5p30</th>\n",
       "      <td>[41, 98, 129, 85, 123, 121, 95, 108, 32, 84, 9...</td>\n",
       "      <td>[41, 41, 41, 40, 39, 38, 37, 31, 29, 26, 20, 18]</td>\n",
       "      <td>[41, 98, 129, 85, 123, 121, 95, 108, 32, 84, 91]</td>\n",
       "      <td>[98, 129, 85, 123, 121, 95, 108, 32, 84, 91, 53]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3127 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             model  \\\n",
       "account_id                                                           \n",
       "1900000215889                                       [131, 30, 107]   \n",
       "0010o00002AGLLq                                      [47, 121, 85]   \n",
       "0010o00002AGU1E                                      [116, 41, 98]   \n",
       "0010o00002AGUKz                                      [116, 41, 98]   \n",
       "0010o00002AGYLD                                 [41, 53, 116, 116]   \n",
       "...                                                            ...   \n",
       "0019000002ACnHZ                                      [106, 99, 25]   \n",
       "0019000002ACnby                                     [12, 107, 131]   \n",
       "0019000002ACsYC                      [41, 53, 116, 41, 41, 41, 41]   \n",
       "0019000002B5m8N             [41, 41, 97, 84, 54, 91, 54, 99, 3, 3]   \n",
       "0019000002B5p30  [41, 98, 129, 85, 123, 121, 95, 108, 32, 84, 9...   \n",
       "\n",
       "                                                          recency  \\\n",
       "account_id                                                          \n",
       "1900000215889                                        [50, 50, 48]   \n",
       "0010o00002AGLLq                                      [41, 40, 37]   \n",
       "0010o00002AGU1E                                       [40, 40, 4]   \n",
       "0010o00002AGUKz                                      [41, 41, 41]   \n",
       "0010o00002AGYLD                                  [40, 18, 18, 10]   \n",
       "...                                                           ...   \n",
       "0019000002ACnHZ                                      [47, 41, 16]   \n",
       "0019000002ACnby                                      [41, 41, 36]   \n",
       "0019000002ACsYC                          [41, 16, 11, 1, 1, 1, 1]   \n",
       "0019000002B5m8N          [47, 46, 46, 36, 36, 32, 32, 28, 13, 11]   \n",
       "0019000002B5p30  [41, 41, 41, 40, 39, 38, 37, 31, 29, 26, 20, 18]   \n",
       "\n",
       "                                                training_sequence  \\\n",
       "account_id                                                          \n",
       "1900000215889                                           [131, 30]   \n",
       "0010o00002AGLLq                                         [47, 121]   \n",
       "0010o00002AGU1E                                         [116, 41]   \n",
       "0010o00002AGUKz                                         [116, 41]   \n",
       "0010o00002AGYLD                                     [41, 53, 116]   \n",
       "...                                                           ...   \n",
       "0019000002ACnHZ                                         [106, 99]   \n",
       "0019000002ACnby                                         [12, 107]   \n",
       "0019000002ACsYC                         [41, 53, 116, 41, 41, 41]   \n",
       "0019000002B5m8N               [41, 41, 97, 84, 54, 91, 54, 99, 3]   \n",
       "0019000002B5p30  [41, 98, 129, 85, 123, 121, 95, 108, 32, 84, 91]   \n",
       "\n",
       "                                                  target_sequence  \n",
       "account_id                                                         \n",
       "1900000215889                                           [30, 107]  \n",
       "0010o00002AGLLq                                         [121, 85]  \n",
       "0010o00002AGU1E                                          [41, 98]  \n",
       "0010o00002AGUKz                                          [41, 98]  \n",
       "0010o00002AGYLD                                    [53, 116, 116]  \n",
       "...                                                           ...  \n",
       "0019000002ACnHZ                                          [99, 25]  \n",
       "0019000002ACnby                                        [107, 131]  \n",
       "0019000002ACsYC                         [53, 116, 41, 41, 41, 41]  \n",
       "0019000002B5m8N                [41, 97, 84, 54, 91, 54, 99, 3, 3]  \n",
       "0019000002B5p30  [98, 129, 85, 123, 121, 95, 108, 32, 84, 91, 53]  \n",
       "\n",
       "[3127 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "779cd861",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(input_data, test_size=0.1, random_state=42)\n",
    "train, val = train_test_split(train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cb6c5242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding sequences to maximum length\n",
    "maxlen = input_data['model'].apply(lambda x: len(x)).sort_values(ascending=False).iloc[0]\n",
    "\n",
    "train_feat_dict = {'training_sequence':pad_sequences(train.training_sequence, maxlen=maxlen, padding='pre', value=0),\n",
    "                    'recency':pad_sequences(train.recency, maxlen=maxlen, padding='pre', value=0)}\n",
    "train_target_tensor = pad_sequences(train.target_sequence, maxlen=maxlen, padding='pre', value=0)\n",
    "\n",
    "\n",
    "val_feat_dict = {'training_sequence':pad_sequences(val.training_sequence, maxlen=maxlen, padding='pre', value=0),\n",
    "                    'recency':pad_sequences(val.recency, maxlen=maxlen, padding='pre', value=0)}\n",
    "val_target_tensor = pad_sequences(val.target_sequence, maxlen=maxlen, padding='pre', value=0)\n",
    "\n",
    "\n",
    "test_feat_dict = {'training_sequence':pad_sequences(test.training_sequence, maxlen=maxlen, padding='pre', value=0),\n",
    "                    'recency':pad_sequences(test.recency, maxlen=maxlen, padding='pre', value=0)}\n",
    "test_target_tensor = pad_sequences(test.target_sequence, maxlen=maxlen, padding='pre', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b51be23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfdata(feat_dict, target_tensor, batch_size, buffer_size=None):\n",
    "    \"\"\"\n",
    "    Create train tf dataset for model train input\n",
    "    :param train_feat_dict: dict, containing the features tensors for train data\n",
    "    :param train_target_tensor: np.array(), the training TARGET tensor\n",
    "    :param batch_size: (int) size of the batch to work with\n",
    "    :param buffer_size: (int) Optional. Default is None. Size of the buffer\n",
    "    :return: (tuple) 1st element is the training dataset,\n",
    "                     2nd is the number of steps per epoch (based on batch size)\n",
    "    \"\"\"\n",
    "    if buffer_size is None:\n",
    "        buffer_size = batch_size*50\n",
    "\n",
    "    steps_per_epoch = len(target_tensor) // batch_size\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((feat_dict, target_tensor)).cache()\n",
    "    dataset = dataset.shuffle(buffer_size).batch(batch_size)\n",
    "    dataset = dataset.repeat().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset, steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1052b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, train_steps_per_epoch = create_tfdata(train_feat_dict,\n",
    "                                                         train_target_tensor,\n",
    "                                                         batch_size=64)\n",
    "\n",
    "val_dataset, val_steps_per_epoch = create_tfdata(val_feat_dict,\n",
    "                                                         val_target_tensor,\n",
    "                                                         batch_size=64)\n",
    "\n",
    "test_dataset, test_steps_per_epoch = create_tfdata(test_feat_dict,\n",
    "                                                         test_target_tensor,\n",
    "                                                         batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d84d6cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_model = max([max(i) for i in input_data['model']])+1\n",
    "max_recency = max([max(i) for i in input_data['recency']])+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4b3311de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(maxlen=maxlen, max_model=max_model, max_recency=max_recency):\n",
    "    \"\"\"\n",
    "    Build a model given the hyper-parameters with item and nb_days input features\n",
    "    :param hp: (kt.HyperParameters) hyper-parameters to use when building this model\n",
    "    :return: built and compiled tensorflow model \n",
    "    \"\"\"\n",
    "    inputs = {}\n",
    "    inputs['training_sequence'] = tf.keras.Input(batch_input_shape=[None, maxlen],\n",
    "                                       name='training_sequence', dtype=tf.int32)\n",
    "    # create encoding padding mask\n",
    "    encoding_padding_mask = tf.math.logical_not(tf.math.equal(inputs['training_sequence'], 0))\n",
    "\n",
    "    # nb_days bucketized\n",
    "    inputs['recency'] = tf.keras.Input(batch_input_shape=[None, maxlen],\n",
    "                                       name='recency', dtype=tf.int32)\n",
    "\n",
    "    # Pass categorical input through embedding layer\n",
    "    # with size equals to tokenizer vocabulary size\n",
    "    # Remember that vocab_size is len of item tokenizer + 1\n",
    "    # (for the padding '0' value)\n",
    "    \n",
    "    embedding_training_sequence = tf.keras.layers.Embedding(input_dim=max_model,\n",
    "                                               output_dim=32,\n",
    "                                               name='embedding_item'\n",
    "                                              )(inputs['training_sequence'])\n",
    "    # nbins=100, +1 for zero padding\n",
    "    embedding_recency = tf.keras.layers.Embedding(input_dim=max_recency,\n",
    "                                                  output_dim=32,\n",
    "                                                  name='embedding_recency'\n",
    "                                                 )(inputs['recency'])\n",
    "\n",
    "    #  Concatenate embedding layers\n",
    "    concat_embedding_input = tf.keras.layers.Concatenate(\n",
    "     name='concat_embedding_input')([embedding_training_sequence, embedding_recency])\n",
    "\n",
    "    concat_embedding_input = tf.keras.layers.BatchNormalization(\n",
    "     name='batchnorm_inputs')(concat_embedding_input)\n",
    "    \n",
    "    # LSTM layer\n",
    "    rnn = tf.keras.layers.LSTM(units=128,\n",
    "                                   return_sequences=True,\n",
    "                                   stateful=False,\n",
    "                                   recurrent_initializer='glorot_normal',\n",
    "                                   name='LSTM_cat'\n",
    "                                   )(concat_embedding_input)\n",
    "\n",
    "    rnn = tf.keras.layers.BatchNormalization(name='batchnorm_lstm')(rnn)\n",
    "\n",
    "    # Self attention so key=value in inputs\n",
    "    att = tf.keras.layers.Attention(use_scale=False, causal=True,\n",
    "                                    name='attention')(inputs=[rnn, rnn],\n",
    "                                                      mask=[encoding_padding_mask,\n",
    "                                                            encoding_padding_mask])\n",
    "\n",
    "    # Last layer is a fully connected one\n",
    "    output = tf.keras.layers.Dense(max_model, name='output', activation='softmax')(att)\n",
    "\n",
    "    model = tf.keras.Model(inputs, output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=loss_function,\n",
    "        metrics=['sparse_categorical_accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4708010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    \"\"\"\n",
    "    We redefine our own loss function in order to get rid of the '0' value\n",
    "    which is the one used for padding. This to avoid that the model optimize itself\n",
    "    by predicting this value because it is the padding one.\n",
    "    \n",
    "    :param real: the truth\n",
    "    :param pred: predictions\n",
    "    :return: a masked loss where '0' in real (due to padding)\n",
    "                are not taken into account for the evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    # to check that pred is numric and not nan\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_object_ = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                                 reduction='none')\n",
    "    loss_ = loss_object_(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fd43204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bd65ae67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "training_sequence (InputLayer)  [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "recency (InputLayer)            [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_item (Embedding)      (None, 15, 32)       4480        training_sequence[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "embedding_recency (Embedding)   (None, 15, 32)       1984        recency[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concat_embedding_input (Concate (None, 15, 64)       0           embedding_item[0][0]             \n",
      "                                                                 embedding_recency[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_inputs (BatchNormaliz (None, 15, 64)       256         concat_embedding_input[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "LSTM_cat (LSTM)                 (None, 15, 128)      98816       batchnorm_inputs[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.equal_3 (TFOpLambda)    (None, 15)           0           training_sequence[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_lstm (BatchNormalizat (None, 15, 128)      512         LSTM_cat[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.logical_not_3 (TFOpLamb (None, 15)           0           tf.math.equal_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention (Attention)           (None, 15, 128)      0           batchnorm_lstm[0][0]             \n",
      "                                                                 batchnorm_lstm[0][0]             \n",
      "                                                                 tf.math.logical_not_3[0][0]      \n",
      "                                                                 tf.math.logical_not_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 15, 140)      18060       attention[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 124,108\n",
      "Trainable params: 123,724\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "16f31d2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-03 15:05:24.279665: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 2591 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.3.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 12582912 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 3s 43ms/step - loss: 0.2146 - sparse_categorical_accuracy: 0.0813 - val_loss: 0.2316 - val_sparse_categorical_accuracy: 0.0453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-03 15:05:26.051155: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 2591 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.3.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 12582912 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000\n",
      "40/40 [==============================] - 1s 32ms/step - loss: 0.1657 - sparse_categorical_accuracy: 0.1674 - val_loss: 0.2301 - val_sparse_categorical_accuracy: 0.0731\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - 1s 32ms/step - loss: 0.1577 - sparse_categorical_accuracy: 0.2064 - val_loss: 0.2262 - val_sparse_categorical_accuracy: 0.0548\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - 1s 32ms/step - loss: 0.1419 - sparse_categorical_accuracy: 0.2453 - val_loss: 0.2253 - val_sparse_categorical_accuracy: 0.0897\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - 1s 33ms/step - loss: 0.1394 - sparse_categorical_accuracy: 0.2587 - val_loss: 0.2196 - val_sparse_categorical_accuracy: 0.0662\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - 1s 32ms/step - loss: 0.1323 - sparse_categorical_accuracy: 0.2872 - val_loss: 0.2191 - val_sparse_categorical_accuracy: 0.1022\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - 1s 34ms/step - loss: 0.1254 - sparse_categorical_accuracy: 0.2986 - val_loss: 0.2054 - val_sparse_categorical_accuracy: 0.1201\n",
      "Epoch 8/1000\n",
      "40/40 [==============================] - 1s 30ms/step - loss: 0.1185 - sparse_categorical_accuracy: 0.3314 - val_loss: 0.2065 - val_sparse_categorical_accuracy: 0.1203\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - 1s 29ms/step - loss: 0.1129 - sparse_categorical_accuracy: 0.3610 - val_loss: 0.1945 - val_sparse_categorical_accuracy: 0.1283\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - 1s 32ms/step - loss: 0.1063 - sparse_categorical_accuracy: 0.3935 - val_loss: 0.1846 - val_sparse_categorical_accuracy: 0.1347\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - 1s 32ms/step - loss: 0.1012 - sparse_categorical_accuracy: 0.4239 - val_loss: 0.1731 - val_sparse_categorical_accuracy: 0.1535\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - 1s 29ms/step - loss: 0.0973 - sparse_categorical_accuracy: 0.4615 - val_loss: 0.1689 - val_sparse_categorical_accuracy: 0.1515\n",
      "Epoch 13/1000\n",
      "40/40 [==============================] - 1s 31ms/step - loss: 0.0864 - sparse_categorical_accuracy: 0.4855 - val_loss: 0.1622 - val_sparse_categorical_accuracy: 0.1483\n",
      "Epoch 14/1000\n",
      "40/40 [==============================] - 1s 31ms/step - loss: 0.0847 - sparse_categorical_accuracy: 0.5217 - val_loss: 0.1687 - val_sparse_categorical_accuracy: 0.1571\n",
      "Epoch 15/1000\n",
      "40/40 [==============================] - 1s 31ms/step - loss: 0.0775 - sparse_categorical_accuracy: 0.5612 - val_loss: 0.1662 - val_sparse_categorical_accuracy: 0.1602\n",
      "Epoch 16/1000\n",
      "40/40 [==============================] - 1s 30ms/step - loss: 0.0742 - sparse_categorical_accuracy: 0.5837 - val_loss: 0.1718 - val_sparse_categorical_accuracy: 0.1583\n",
      "Epoch 17/1000\n",
      "40/40 [==============================] - 1s 33ms/step - loss: 0.0684 - sparse_categorical_accuracy: 0.6219 - val_loss: 0.1867 - val_sparse_categorical_accuracy: 0.1529\n",
      "Epoch 18/1000\n",
      "40/40 [==============================] - 1s 32ms/step - loss: 0.0633 - sparse_categorical_accuracy: 0.6472 - val_loss: 0.1996 - val_sparse_categorical_accuracy: 0.1537\n",
      "Epoch 19/1000\n",
      "40/40 [==============================] - 1s 32ms/step - loss: 0.0579 - sparse_categorical_accuracy: 0.6795 - val_loss: 0.2075 - val_sparse_categorical_accuracy: 0.1326\n",
      "Epoch 20/1000\n",
      "40/40 [==============================] - 1s 32ms/step - loss: 0.0565 - sparse_categorical_accuracy: 0.7003 - val_loss: 0.2152 - val_sparse_categorical_accuracy: 0.1470\n",
      "Epoch 21/1000\n",
      "40/40 [==============================] - 1s 32ms/step - loss: 0.0498 - sparse_categorical_accuracy: 0.7301 - val_loss: 0.2314 - val_sparse_categorical_accuracy: 0.1402\n",
      "Epoch 22/1000\n",
      "40/40 [==============================] - 1s 32ms/step - loss: 0.0470 - sparse_categorical_accuracy: 0.7609 - val_loss: 0.2209 - val_sparse_categorical_accuracy: 0.1281\n",
      "Epoch 23/1000\n",
      "40/40 [==============================] - 1s 32ms/step - loss: 0.0413 - sparse_categorical_accuracy: 0.7760 - val_loss: 0.2276 - val_sparse_categorical_accuracy: 0.1310\n",
      "Epoch 24/1000\n",
      "40/40 [==============================] - 1s 32ms/step - loss: 0.0389 - sparse_categorical_accuracy: 0.7910 - val_loss: 0.2379 - val_sparse_categorical_accuracy: 0.1291\n",
      "Epoch 25/1000\n",
      "40/40 [==============================] - 1s 32ms/step - loss: 0.0385 - sparse_categorical_accuracy: 0.8069 - val_loss: 0.2486 - val_sparse_categorical_accuracy: 0.1353\n",
      "Epoch 26/1000\n",
      "40/40 [==============================] - 1s 32ms/step - loss: 0.0347 - sparse_categorical_accuracy: 0.8117 - val_loss: 0.2572 - val_sparse_categorical_accuracy: 0.1246\n",
      "Epoch 27/1000\n",
      "40/40 [==============================] - 1s 32ms/step - loss: 0.0328 - sparse_categorical_accuracy: 0.8358 - val_loss: 0.2593 - val_sparse_categorical_accuracy: 0.1273\n",
      "Epoch 28/1000\n",
      "40/40 [==============================] - 1s 33ms/step - loss: 0.0285 - sparse_categorical_accuracy: 0.8569 - val_loss: 0.2597 - val_sparse_categorical_accuracy: 0.1298\n",
      "Epoch 29/1000\n",
      "40/40 [==============================] - 1s 33ms/step - loss: 0.0267 - sparse_categorical_accuracy: 0.8628 - val_loss: 0.2801 - val_sparse_categorical_accuracy: 0.1290\n",
      "Epoch 30/1000\n",
      "40/40 [==============================] - 1s 33ms/step - loss: 0.0254 - sparse_categorical_accuracy: 0.8740 - val_loss: 0.2715 - val_sparse_categorical_accuracy: 0.1175\n",
      "Epoch 31/1000\n",
      "40/40 [==============================] - 1s 30ms/step - loss: 0.0229 - sparse_categorical_accuracy: 0.8804 - val_loss: 0.2754 - val_sparse_categorical_accuracy: 0.1303\n",
      "Epoch 32/1000\n",
      "40/40 [==============================] - 1s 31ms/step - loss: 0.0228 - sparse_categorical_accuracy: 0.8843 - val_loss: 0.2798 - val_sparse_categorical_accuracy: 0.1249\n",
      "Epoch 33/1000\n",
      "40/40 [==============================] - 1s 31ms/step - loss: 0.0222 - sparse_categorical_accuracy: 0.8835 - val_loss: 0.2867 - val_sparse_categorical_accuracy: 0.1288\n",
      "Epoch 34/1000\n",
      "40/40 [==============================] - 1s 32ms/step - loss: 0.0206 - sparse_categorical_accuracy: 0.8955 - val_loss: 0.2927 - val_sparse_categorical_accuracy: 0.1170\n",
      "Epoch 35/1000\n",
      "40/40 [==============================] - 1s 34ms/step - loss: 0.0188 - sparse_categorical_accuracy: 0.9008 - val_loss: 0.2972 - val_sparse_categorical_accuracy: 0.1225\n",
      "Epoch 36/1000\n",
      "40/40 [==============================] - 1s 29ms/step - loss: 0.0180 - sparse_categorical_accuracy: 0.9026 - val_loss: 0.2973 - val_sparse_categorical_accuracy: 0.1171\n",
      "Epoch 37/1000\n",
      "40/40 [==============================] - 1s 30ms/step - loss: 0.0166 - sparse_categorical_accuracy: 0.9083 - val_loss: 0.3039 - val_sparse_categorical_accuracy: 0.1218\n",
      "Epoch 38/1000\n",
      "40/40 [==============================] - 1s 28ms/step - loss: 0.0166 - sparse_categorical_accuracy: 0.9097 - val_loss: 0.3115 - val_sparse_categorical_accuracy: 0.1242\n",
      "Epoch 39/1000\n",
      "40/40 [==============================] - 1s 31ms/step - loss: 0.0155 - sparse_categorical_accuracy: 0.9147 - val_loss: 0.3215 - val_sparse_categorical_accuracy: 0.1187\n",
      "Epoch 40/1000\n",
      "40/40 [==============================] - 1s 32ms/step - loss: 0.0159 - sparse_categorical_accuracy: 0.9099 - val_loss: 0.3069 - val_sparse_categorical_accuracy: 0.1195\n",
      "Epoch 41/1000\n",
      "40/40 [==============================] - 1s 31ms/step - loss: 0.0151 - sparse_categorical_accuracy: 0.9138 - val_loss: 0.3173 - val_sparse_categorical_accuracy: 0.1240\n",
      "Epoch 42/1000\n",
      "40/40 [==============================] - 1s 31ms/step - loss: 0.0143 - sparse_categorical_accuracy: 0.9147 - val_loss: 0.3136 - val_sparse_categorical_accuracy: 0.1193\n",
      "Epoch 43/1000\n",
      "40/40 [==============================] - 1s 32ms/step - loss: 0.0143 - sparse_categorical_accuracy: 0.9109 - val_loss: 0.3158 - val_sparse_categorical_accuracy: 0.1188\n",
      "Epoch 44/1000\n",
      "40/40 [==============================] - 1s 30ms/step - loss: 0.0140 - sparse_categorical_accuracy: 0.9182 - val_loss: 0.3284 - val_sparse_categorical_accuracy: 0.1228\n",
      "Epoch 45/1000\n",
      "40/40 [==============================] - 1s 32ms/step - loss: 0.0136 - sparse_categorical_accuracy: 0.9152 - val_loss: 0.3248 - val_sparse_categorical_accuracy: 0.1195\n",
      "Epoch 46/1000\n",
      "40/40 [==============================] - 1s 32ms/step - loss: 0.0134 - sparse_categorical_accuracy: 0.9201 - val_loss: 0.3384 - val_sparse_categorical_accuracy: 0.1185\n",
      "Epoch 47/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 1s 31ms/step - loss: 0.0133 - sparse_categorical_accuracy: 0.9200 - val_loss: 0.3333 - val_sparse_categorical_accuracy: 0.1295\n",
      "Epoch 48/1000\n",
      "40/40 [==============================] - 1s 33ms/step - loss: 0.0133 - sparse_categorical_accuracy: 0.9136 - val_loss: 0.3391 - val_sparse_categorical_accuracy: 0.1196\n",
      "Epoch 49/1000\n",
      "40/40 [==============================] - 1s 31ms/step - loss: 0.0132 - sparse_categorical_accuracy: 0.9190 - val_loss: 0.3387 - val_sparse_categorical_accuracy: 0.1207\n",
      "Epoch 50/1000\n",
      "40/40 [==============================] - 1s 31ms/step - loss: 0.0123 - sparse_categorical_accuracy: 0.9210 - val_loss: 0.3378 - val_sparse_categorical_accuracy: 0.1167\n",
      "Epoch 51/1000\n",
      "40/40 [==============================] - 1s 31ms/step - loss: 0.0126 - sparse_categorical_accuracy: 0.9198 - val_loss: 0.3396 - val_sparse_categorical_accuracy: 0.1261\n",
      "Epoch 52/1000\n",
      "40/40 [==============================] - 1s 31ms/step - loss: 0.0118 - sparse_categorical_accuracy: 0.9219 - val_loss: 0.3462 - val_sparse_categorical_accuracy: 0.1187\n",
      "Epoch 53/1000\n",
      "40/40 [==============================] - 1s 32ms/step - loss: 0.0122 - sparse_categorical_accuracy: 0.9195 - val_loss: 0.3513 - val_sparse_categorical_accuracy: 0.1211\n",
      "Epoch 54/1000\n",
      "40/40 [==============================] - 1s 31ms/step - loss: 0.0114 - sparse_categorical_accuracy: 0.9252 - val_loss: 0.3515 - val_sparse_categorical_accuracy: 0.1142\n",
      "Epoch 55/1000\n",
      "40/40 [==============================] - 1s 31ms/step - loss: 0.0114 - sparse_categorical_accuracy: 0.9222 - val_loss: 0.3484 - val_sparse_categorical_accuracy: 0.1164\n",
      "Epoch 56/1000\n",
      "40/40 [==============================] - 1s 32ms/step - loss: 0.0106 - sparse_categorical_accuracy: 0.9262 - val_loss: 0.3443 - val_sparse_categorical_accuracy: 0.1178\n",
      "Epoch 57/1000\n",
      "40/40 [==============================] - 1s 30ms/step - loss: 0.0115 - sparse_categorical_accuracy: 0.9231 - val_loss: 0.3494 - val_sparse_categorical_accuracy: 0.1251\n",
      "Epoch 58/1000\n",
      "40/40 [==============================] - 1s 32ms/step - loss: 0.0114 - sparse_categorical_accuracy: 0.9261 - val_loss: 0.3630 - val_sparse_categorical_accuracy: 0.1160\n",
      "Epoch 59/1000\n",
      "40/40 [==============================] - 1s 29ms/step - loss: 0.0109 - sparse_categorical_accuracy: 0.9240 - val_loss: 0.3505 - val_sparse_categorical_accuracy: 0.1136\n",
      "Epoch 60/1000\n",
      "40/40 [==============================] - 1s 29ms/step - loss: 0.0114 - sparse_categorical_accuracy: 0.9251 - val_loss: 0.3525 - val_sparse_categorical_accuracy: 0.1185\n",
      "Epoch 61/1000\n",
      "40/40 [==============================] - 1s 29ms/step - loss: 0.0112 - sparse_categorical_accuracy: 0.9230 - val_loss: 0.3632 - val_sparse_categorical_accuracy: 0.1244\n",
      "Epoch 62/1000\n",
      "40/40 [==============================] - 1s 30ms/step - loss: 0.0105 - sparse_categorical_accuracy: 0.9315 - val_loss: 0.3639 - val_sparse_categorical_accuracy: 0.1166\n",
      "Epoch 63/1000\n",
      "40/40 [==============================] - 1s 31ms/step - loss: 0.0106 - sparse_categorical_accuracy: 0.9241 - val_loss: 0.3564 - val_sparse_categorical_accuracy: 0.1176\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(patience=50, restore_best_weights=True)\n",
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=1000, steps_per_epoch=40, validation_steps=10, callbacks=es, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ebfd3237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 35\n",
    "\n",
    "# test_feat_dict = {'training_sequence': train_feat_dict['training_sequence'][index:index+1],\n",
    "#                 'recency': train_feat_dict['recency'][index:index+1]}\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((test_feat_dict)).cache()\n",
    "\n",
    "# y_pred = model.predict(test_dataset)\n",
    "# output = []\n",
    "# for i in range(15):\n",
    "#     maxElement = np.amax(y_pred[i][0])\n",
    "#     result = np.where(y_pred[i][0] == np.amax(y_pred[i][0]))\n",
    "#     output.append(result[0][0])\n",
    "\n",
    "# print(f'predicted: {output}')\n",
    "# print(f'actual   : {train_target_tensor[index:index+1][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "56b75ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: [121, 121, 121, 121, 121, 121, 121, 121, 121, 121, 121, 121, 121, 75, 41]\n",
      "actual   : [  0   0   0   0   0   0   0   0   0   0   0 130  97  79  49]\n"
     ]
    }
   ],
   "source": [
    "index =40\n",
    "\n",
    "y_pred = model.predict(test_dataset)\n",
    "\n",
    "output = []\n",
    "for i in range(15):\n",
    "    maxElement = np.amax(y_pred[i][0])\n",
    "    result = np.where(y_pred[i][0] == np.amax(y_pred[i][0]))\n",
    "    output.append(result[0][0])\n",
    "\n",
    "print(f'predicted: {output}')\n",
    "print(f'actual   : {test_target_tensor[index:index+1][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "8113a576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8958cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
