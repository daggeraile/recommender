{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "31c7ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e08b062",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"../raw_data/data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "1892c78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering down to essential features & transform datetime feature\n",
    "\n",
    "df = data[['Account: Account ID', 'Product Category (6D)', 'Model  Name', 'Purchase  Date', 'Purchase Month', 'Purchase Year']]\n",
    "df['Purchase  Date'] = pd.to_datetime(df['Purchase  Date'])\n",
    "df = df[df['Purchase  Date']>'2017-04-01']\n",
    "df['Today'] = datetime.today()\n",
    "df['Recency'] = round((df['Today'] - df['Purchase  Date']).dt.days/30,0)\n",
    "df['Recency'] = df['Recency'].astype('int')\n",
    "df.drop(columns=['Purchase  Date', 'Purchase Month', 'Purchase Year', 'Today'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "d5a7127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering down to customers with purchase history between 3~15\n",
    "df_count = df.groupby('Account: Account ID').count()[['Model  Name']]\n",
    "df_count = df_count[df_count['Model  Name']>=3]\n",
    "df_count = df_count[df_count['Model  Name']<=15]\n",
    "customer_id = df_count.index\n",
    "input_data = df[df['Account: Account ID'].isin(customer_id)]\n",
    "input_data = input_data.rename(columns={'Account: Account ID':'account_id', 'Model  Name':'model', 'Recency':'recency'})[['account_id','model', 'recency']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "998d048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelEncoder model into integers\n",
    "encoder = LabelEncoder()\n",
    "input_data['model'] = encoder.fit_transform(input_data['model'])\n",
    "input_data['model'] = input_data['model'].apply(lambda x: x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "3ee29f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby customer, aggregate model and recency into arrays\n",
    "input_data = input_data.groupby('account_id').agg(list)\n",
    "input_data['model'] = input_data['model'].apply(lambda x: np.array(x))\n",
    "input_data['recency'] = input_data['recency'].apply(lambda x: np.array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "bc7bb92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating training_sequence & target_sequence\n",
    "input_data['training_sequence'] = input_data['model'].apply(lambda x: x[:-1])\n",
    "input_data['target_sequence'] = input_data['model'].apply(lambda x: x[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "5ddd02c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>recency</th>\n",
       "      <th>training_sequence</th>\n",
       "      <th>target_sequence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>account_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1900000215889</th>\n",
       "      <td>[131, 30, 107]</td>\n",
       "      <td>[50, 50, 48]</td>\n",
       "      <td>[131, 30]</td>\n",
       "      <td>[30, 107]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0010o00002AGLLq</th>\n",
       "      <td>[47, 121, 85]</td>\n",
       "      <td>[41, 40, 37]</td>\n",
       "      <td>[47, 121]</td>\n",
       "      <td>[121, 85]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0010o00002AGU1E</th>\n",
       "      <td>[116, 41, 98]</td>\n",
       "      <td>[40, 40, 4]</td>\n",
       "      <td>[116, 41]</td>\n",
       "      <td>[41, 98]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0010o00002AGUKz</th>\n",
       "      <td>[116, 41, 98]</td>\n",
       "      <td>[41, 41, 41]</td>\n",
       "      <td>[116, 41]</td>\n",
       "      <td>[41, 98]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0010o00002AGYLD</th>\n",
       "      <td>[41, 53, 116, 116]</td>\n",
       "      <td>[40, 18, 18, 10]</td>\n",
       "      <td>[41, 53, 116]</td>\n",
       "      <td>[53, 116, 116]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0019000002ACnHZ</th>\n",
       "      <td>[106, 99, 25]</td>\n",
       "      <td>[47, 41, 16]</td>\n",
       "      <td>[106, 99]</td>\n",
       "      <td>[99, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0019000002ACnby</th>\n",
       "      <td>[12, 107, 131]</td>\n",
       "      <td>[41, 41, 36]</td>\n",
       "      <td>[12, 107]</td>\n",
       "      <td>[107, 131]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0019000002ACsYC</th>\n",
       "      <td>[41, 53, 116, 41, 41, 41, 41]</td>\n",
       "      <td>[41, 16, 11, 1, 1, 1, 1]</td>\n",
       "      <td>[41, 53, 116, 41, 41, 41]</td>\n",
       "      <td>[53, 116, 41, 41, 41, 41]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0019000002B5m8N</th>\n",
       "      <td>[41, 41, 97, 84, 54, 91, 54, 99, 3, 3]</td>\n",
       "      <td>[47, 46, 46, 36, 36, 32, 32, 28, 12, 11]</td>\n",
       "      <td>[41, 41, 97, 84, 54, 91, 54, 99, 3]</td>\n",
       "      <td>[41, 97, 84, 54, 91, 54, 99, 3, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0019000002B5p30</th>\n",
       "      <td>[41, 98, 129, 85, 123, 121, 95, 108, 32, 84, 9...</td>\n",
       "      <td>[41, 41, 41, 40, 39, 38, 37, 31, 29, 26, 20, 18]</td>\n",
       "      <td>[41, 98, 129, 85, 123, 121, 95, 108, 32, 84, 91]</td>\n",
       "      <td>[98, 129, 85, 123, 121, 95, 108, 32, 84, 91, 53]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3127 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             model  \\\n",
       "account_id                                                           \n",
       "1900000215889                                       [131, 30, 107]   \n",
       "0010o00002AGLLq                                      [47, 121, 85]   \n",
       "0010o00002AGU1E                                      [116, 41, 98]   \n",
       "0010o00002AGUKz                                      [116, 41, 98]   \n",
       "0010o00002AGYLD                                 [41, 53, 116, 116]   \n",
       "...                                                            ...   \n",
       "0019000002ACnHZ                                      [106, 99, 25]   \n",
       "0019000002ACnby                                     [12, 107, 131]   \n",
       "0019000002ACsYC                      [41, 53, 116, 41, 41, 41, 41]   \n",
       "0019000002B5m8N             [41, 41, 97, 84, 54, 91, 54, 99, 3, 3]   \n",
       "0019000002B5p30  [41, 98, 129, 85, 123, 121, 95, 108, 32, 84, 9...   \n",
       "\n",
       "                                                          recency  \\\n",
       "account_id                                                          \n",
       "1900000215889                                        [50, 50, 48]   \n",
       "0010o00002AGLLq                                      [41, 40, 37]   \n",
       "0010o00002AGU1E                                       [40, 40, 4]   \n",
       "0010o00002AGUKz                                      [41, 41, 41]   \n",
       "0010o00002AGYLD                                  [40, 18, 18, 10]   \n",
       "...                                                           ...   \n",
       "0019000002ACnHZ                                      [47, 41, 16]   \n",
       "0019000002ACnby                                      [41, 41, 36]   \n",
       "0019000002ACsYC                          [41, 16, 11, 1, 1, 1, 1]   \n",
       "0019000002B5m8N          [47, 46, 46, 36, 36, 32, 32, 28, 12, 11]   \n",
       "0019000002B5p30  [41, 41, 41, 40, 39, 38, 37, 31, 29, 26, 20, 18]   \n",
       "\n",
       "                                                training_sequence  \\\n",
       "account_id                                                          \n",
       "1900000215889                                           [131, 30]   \n",
       "0010o00002AGLLq                                         [47, 121]   \n",
       "0010o00002AGU1E                                         [116, 41]   \n",
       "0010o00002AGUKz                                         [116, 41]   \n",
       "0010o00002AGYLD                                     [41, 53, 116]   \n",
       "...                                                           ...   \n",
       "0019000002ACnHZ                                         [106, 99]   \n",
       "0019000002ACnby                                         [12, 107]   \n",
       "0019000002ACsYC                         [41, 53, 116, 41, 41, 41]   \n",
       "0019000002B5m8N               [41, 41, 97, 84, 54, 91, 54, 99, 3]   \n",
       "0019000002B5p30  [41, 98, 129, 85, 123, 121, 95, 108, 32, 84, 91]   \n",
       "\n",
       "                                                  target_sequence  \n",
       "account_id                                                         \n",
       "1900000215889                                           [30, 107]  \n",
       "0010o00002AGLLq                                         [121, 85]  \n",
       "0010o00002AGU1E                                          [41, 98]  \n",
       "0010o00002AGUKz                                          [41, 98]  \n",
       "0010o00002AGYLD                                    [53, 116, 116]  \n",
       "...                                                           ...  \n",
       "0019000002ACnHZ                                          [99, 25]  \n",
       "0019000002ACnby                                        [107, 131]  \n",
       "0019000002ACsYC                         [53, 116, 41, 41, 41, 41]  \n",
       "0019000002B5m8N                [41, 97, 84, 54, 91, 54, 99, 3, 3]  \n",
       "0019000002B5p30  [98, 129, 85, 123, 121, 95, 108, 32, 84, 91, 53]  \n",
       "\n",
       "[3127 rows x 4 columns]"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "cb6c5242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding sequences to maximum length\n",
    "maxlen = input_data['model'].apply(lambda x: len(x)).sort_values(ascending=False).iloc[0]\n",
    "\n",
    "train_feat_dict = {'training_sequence':pad_sequences(input_data.training_sequence, maxlen=maxlen, padding='pre', value=0),\n",
    "                    'recency':pad_sequences(input_data.recency, maxlen=maxlen, padding='pre', value=0)}\n",
    "train_target_tensor = pad_sequences(input_data.target_sequence, maxlen=maxlen, padding='pre', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "b51be23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_tfdata(train_feat_dict, train_target_tensor,\n",
    "                        batch_size, buffer_size=None):\n",
    "    \"\"\"\n",
    "    Create train tf dataset for model train input\n",
    "    :param train_feat_dict: dict, containing the features tensors for train data\n",
    "    :param train_target_tensor: np.array(), the training TARGET tensor\n",
    "    :param batch_size: (int) size of the batch to work with\n",
    "    :param buffer_size: (int) Optional. Default is None. Size of the buffer\n",
    "    :return: (tuple) 1st element is the training dataset,\n",
    "                     2nd is the number of steps per epoch (based on batch size)\n",
    "    \"\"\"\n",
    "    if buffer_size is None:\n",
    "        buffer_size = batch_size*50\n",
    "\n",
    "    train_steps_per_epoch = len(train_target_tensor) // batch_size\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_feat_dict,\n",
    "                                                        train_target_tensor)).cache()\n",
    "    train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size)\n",
    "    train_dataset = train_dataset.repeat().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return train_dataset, train_steps_per_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "1052b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, train_steps_per_epoch = create_train_tfdata(train_feat_dict,\n",
    "                                                         train_target_tensor,\n",
    "                                                         batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "d84d6cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_model = max([max(i) for i in input_data['model']])+1\n",
    "max_recency = max([max(i) for i in input_data['recency']])+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "4b3311de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(maxlen=maxlen, max_model=max_model, max_recency=max_recency):\n",
    "    \"\"\"\n",
    "    Build a model given the hyper-parameters with item and nb_days input features\n",
    "    :param hp: (kt.HyperParameters) hyper-parameters to use when building this model\n",
    "    :return: built and compiled tensorflow model \n",
    "    \"\"\"\n",
    "    inputs = {}\n",
    "    inputs['training_sequence'] = tf.keras.Input(batch_input_shape=[None, maxlen],\n",
    "                                       name='training_sequence', dtype=tf.int32)\n",
    "    # create encoding padding mask\n",
    "    encoding_padding_mask = tf.math.logical_not(tf.math.equal(inputs['training_sequence'], 0))\n",
    "\n",
    "    # nb_days bucketized\n",
    "    inputs['recency'] = tf.keras.Input(batch_input_shape=[None, maxlen],\n",
    "                                       name='recency', dtype=tf.int32)\n",
    "\n",
    "    # Pass categorical input through embedding layer\n",
    "    # with size equals to tokenizer vocabulary size\n",
    "    # Remember that vocab_size is len of item tokenizer + 1\n",
    "    # (for the padding '0' value)\n",
    "    \n",
    "    embedding_training_sequence = tf.keras.layers.Embedding(input_dim=max_model,\n",
    "                                               output_dim=10,\n",
    "                                               name='embedding_item'\n",
    "                                              )(inputs['training_sequence'])\n",
    "    # nbins=100, +1 for zero padding\n",
    "    embedding_recency = tf.keras.layers.Embedding(input_dim=max_recency,\n",
    "                                                  output_dim=10,\n",
    "                                                  name='embedding_recency'\n",
    "                                                 )(inputs['recency'])\n",
    "\n",
    "    #  Concatenate embedding layers\n",
    "    concat_embedding_input = tf.keras.layers.Concatenate(\n",
    "     name='concat_embedding_input')([embedding_training_sequence, embedding_recency])\n",
    "\n",
    "    concat_embedding_input = tf.keras.layers.BatchNormalization(\n",
    "     name='batchnorm_inputs')(concat_embedding_input)\n",
    "    \n",
    "    # LSTM layer\n",
    "    rnn = tf.keras.layers.LSTM(units=32,\n",
    "                                   return_sequences=True,\n",
    "                                   stateful=False,\n",
    "                                   recurrent_initializer='glorot_normal',\n",
    "                                   name='LSTM_cat'\n",
    "                                   )(concat_embedding_input)\n",
    "\n",
    "    rnn = tf.keras.layers.BatchNormalization(name='batchnorm_lstm')(rnn)\n",
    "\n",
    "    # Self attention so key=value in inputs\n",
    "    att = tf.keras.layers.Attention(use_scale=False, causal=True,\n",
    "                                    name='attention')(inputs=[rnn, rnn],\n",
    "                                                      mask=[encoding_padding_mask,\n",
    "                                                            encoding_padding_mask])\n",
    "\n",
    "    # Last layer is a fully connected one\n",
    "    output = tf.keras.layers.Dense(max_model, name='output')(att)\n",
    "\n",
    "    model = tf.keras.Model(inputs, output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=loss_function,\n",
    "        metrics=['sparse_categorical_accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "4708010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    \"\"\"\n",
    "    We redefine our own loss function in order to get rid of the '0' value\n",
    "    which is the one used for padding. This to avoid that the model optimize itself\n",
    "    by predicting this value because it is the padding one.\n",
    "    \n",
    "    :param real: the truth\n",
    "    :param pred: predictions\n",
    "    :return: a masked loss where '0' in real (due to padding)\n",
    "                are not taken into account for the evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    # to check that pred is numric and not nan\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_object_ = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                                 reduction='none')\n",
    "    loss_ = loss_object_(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "6e96eb94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ({training_sequence: (None, 15), recency: (None, 15)}, (None, 15)), types: ({training_sequence: tf.int32, recency: tf.int32}, tf.int32)>"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "fd43204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "bd65ae67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "training_sequence (InputLayer)  [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "recency (InputLayer)            [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_item (Embedding)      (None, 15, 10)       1400        training_sequence[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "embedding_recency (Embedding)   (None, 15, 10)       620         recency[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concat_embedding_input (Concate (None, 15, 20)       0           embedding_item[0][0]             \n",
      "                                                                 embedding_recency[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_inputs (BatchNormaliz (None, 15, 20)       80          concat_embedding_input[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "LSTM_cat (LSTM)                 (None, 15, 32)       6784        batchnorm_inputs[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.equal_6 (TFOpLambda)    (None, 15)           0           training_sequence[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_lstm (BatchNormalizat (None, 15, 32)       128         LSTM_cat[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.logical_not_6 (TFOpLamb (None, 15)           0           tf.math.equal_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention (Attention)           (None, 15, 32)       0           batchnorm_lstm[0][0]             \n",
      "                                                                 batchnorm_lstm[0][0]             \n",
      "                                                                 tf.math.logical_not_6[0][0]      \n",
      "                                                                 tf.math.logical_not_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 15, 140)      4620        attention[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 13,632\n",
      "Trainable params: 13,528\n",
      "Non-trainable params: 104\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "16f31d2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.1363 - sparse_categorical_accuracy: 0.2534\n",
      "Epoch 2/100\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.1352 - sparse_categorical_accuracy: 0.2580\n",
      "Epoch 3/100\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.1345 - sparse_categorical_accuracy: 0.2617\n",
      "Epoch 4/100\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.1336 - sparse_categorical_accuracy: 0.2597\n",
      "Epoch 5/100\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.1336 - sparse_categorical_accuracy: 0.2689\n",
      "Epoch 6/100\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.1315 - sparse_categorical_accuracy: 0.2656\n",
      "Epoch 7/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1330 - sparse_categorical_accuracy: 0.2683\n",
      "Epoch 8/100\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.1332 - sparse_categorical_accuracy: 0.2702\n",
      "Epoch 9/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1299 - sparse_categorical_accuracy: 0.2778\n",
      "Epoch 10/100\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.1310 - sparse_categorical_accuracy: 0.2694\n",
      "Epoch 11/100\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.1292 - sparse_categorical_accuracy: 0.2704\n",
      "Epoch 12/100\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.1287 - sparse_categorical_accuracy: 0.2830\n",
      "Epoch 13/100\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.1301 - sparse_categorical_accuracy: 0.2820\n",
      "Epoch 14/100\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.1278 - sparse_categorical_accuracy: 0.2897\n",
      "Epoch 15/100\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.1274 - sparse_categorical_accuracy: 0.2847\n",
      "Epoch 16/100\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.1283 - sparse_categorical_accuracy: 0.2870\n",
      "Epoch 17/100\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.1276 - sparse_categorical_accuracy: 0.2866\n",
      "Epoch 18/100\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.1277 - sparse_categorical_accuracy: 0.2884\n",
      "Epoch 19/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1253 - sparse_categorical_accuracy: 0.2978\n",
      "Epoch 20/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1256 - sparse_categorical_accuracy: 0.2962\n",
      "Epoch 21/100\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.1255 - sparse_categorical_accuracy: 0.2914\n",
      "Epoch 22/100\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.1260 - sparse_categorical_accuracy: 0.2929\n",
      "Epoch 23/100\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.1263 - sparse_categorical_accuracy: 0.3006\n",
      "Epoch 24/100\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.1216 - sparse_categorical_accuracy: 0.2968\n",
      "Epoch 25/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1237 - sparse_categorical_accuracy: 0.2994\n",
      "Epoch 26/100\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.1274 - sparse_categorical_accuracy: 0.3039\n",
      "Epoch 27/100\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.1218 - sparse_categorical_accuracy: 0.3056\n",
      "Epoch 28/100\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.1213 - sparse_categorical_accuracy: 0.3103\n",
      "Epoch 29/100\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.1258 - sparse_categorical_accuracy: 0.3051\n",
      "Epoch 30/100\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.1205 - sparse_categorical_accuracy: 0.3114\n",
      "Epoch 31/100\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.1212 - sparse_categorical_accuracy: 0.3070\n",
      "Epoch 32/100\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.1221 - sparse_categorical_accuracy: 0.3101\n",
      "Epoch 33/100\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.1191 - sparse_categorical_accuracy: 0.3177\n",
      "Epoch 34/100\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.1229 - sparse_categorical_accuracy: 0.3083\n",
      "Epoch 35/100\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.1209 - sparse_categorical_accuracy: 0.3188\n",
      "Epoch 36/100\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.1196 - sparse_categorical_accuracy: 0.3111\n",
      "Epoch 37/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1215 - sparse_categorical_accuracy: 0.3182\n",
      "Epoch 38/100\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.1220 - sparse_categorical_accuracy: 0.3238\n",
      "Epoch 39/100\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.1158 - sparse_categorical_accuracy: 0.3213\n",
      "Epoch 40/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.1214 - sparse_categorical_accuracy: 0.3163\n",
      "Epoch 41/100\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.1173 - sparse_categorical_accuracy: 0.3233\n",
      "Epoch 42/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.1198 - sparse_categorical_accuracy: 0.3261\n",
      "Epoch 43/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.1171 - sparse_categorical_accuracy: 0.3225\n",
      "Epoch 44/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.1185 - sparse_categorical_accuracy: 0.3226\n",
      "Epoch 45/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.1202 - sparse_categorical_accuracy: 0.3285\n",
      "Epoch 46/100\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.1165 - sparse_categorical_accuracy: 0.3243\n",
      "Epoch 47/100\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.1160 - sparse_categorical_accuracy: 0.3282\n",
      "Epoch 48/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1175 - sparse_categorical_accuracy: 0.3309\n",
      "Epoch 49/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1171 - sparse_categorical_accuracy: 0.3310\n",
      "Epoch 50/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1169 - sparse_categorical_accuracy: 0.3354\n",
      "Epoch 51/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1170 - sparse_categorical_accuracy: 0.3326\n",
      "Epoch 52/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1154 - sparse_categorical_accuracy: 0.3361\n",
      "Epoch 53/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1149 - sparse_categorical_accuracy: 0.3413\n",
      "Epoch 54/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1169 - sparse_categorical_accuracy: 0.3386\n",
      "Epoch 55/100\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.1157 - sparse_categorical_accuracy: 0.3364\n",
      "Epoch 56/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1136 - sparse_categorical_accuracy: 0.3419\n",
      "Epoch 57/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1170 - sparse_categorical_accuracy: 0.3415\n",
      "Epoch 58/100\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.1142 - sparse_categorical_accuracy: 0.3398\n",
      "Epoch 59/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1138 - sparse_categorical_accuracy: 0.3448\n",
      "Epoch 60/100\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.1133 - sparse_categorical_accuracy: 0.3410\n",
      "Epoch 61/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1166 - sparse_categorical_accuracy: 0.3469\n",
      "Epoch 62/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1133 - sparse_categorical_accuracy: 0.3549\n",
      "Epoch 63/100\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.1103 - sparse_categorical_accuracy: 0.3493\n",
      "Epoch 64/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1166 - sparse_categorical_accuracy: 0.3479\n",
      "Epoch 65/100\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.1118 - sparse_categorical_accuracy: 0.3508\n",
      "Epoch 66/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1123 - sparse_categorical_accuracy: 0.3541\n",
      "Epoch 67/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1148 - sparse_categorical_accuracy: 0.3533\n",
      "Epoch 68/100\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.1132 - sparse_categorical_accuracy: 0.3471\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1106 - sparse_categorical_accuracy: 0.3572\n",
      "Epoch 70/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1139 - sparse_categorical_accuracy: 0.3493\n",
      "Epoch 71/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1113 - sparse_categorical_accuracy: 0.3506\n",
      "Epoch 72/100\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.1117 - sparse_categorical_accuracy: 0.3573\n",
      "Epoch 73/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1115 - sparse_categorical_accuracy: 0.3579\n",
      "Epoch 74/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1116 - sparse_categorical_accuracy: 0.3621\n",
      "Epoch 75/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1105 - sparse_categorical_accuracy: 0.3609\n",
      "Epoch 76/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1111 - sparse_categorical_accuracy: 0.3539\n",
      "Epoch 77/100\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.1108 - sparse_categorical_accuracy: 0.3675\n",
      "Epoch 78/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1137 - sparse_categorical_accuracy: 0.3611\n",
      "Epoch 79/100\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.1084 - sparse_categorical_accuracy: 0.3595\n",
      "Epoch 80/100\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.1119 - sparse_categorical_accuracy: 0.3589\n",
      "Epoch 81/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1105 - sparse_categorical_accuracy: 0.3618\n",
      "Epoch 82/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1094 - sparse_categorical_accuracy: 0.3719\n",
      "Epoch 83/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1113 - sparse_categorical_accuracy: 0.3638\n",
      "Epoch 84/100\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.1065 - sparse_categorical_accuracy: 0.3608\n",
      "Epoch 85/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1096 - sparse_categorical_accuracy: 0.3762\n",
      "Epoch 86/100\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.1120 - sparse_categorical_accuracy: 0.3654\n",
      "Epoch 87/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1072 - sparse_categorical_accuracy: 0.3773\n",
      "Epoch 88/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1122 - sparse_categorical_accuracy: 0.3675\n",
      "Epoch 89/100\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.1074 - sparse_categorical_accuracy: 0.3722\n",
      "Epoch 90/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1085 - sparse_categorical_accuracy: 0.3707\n",
      "Epoch 91/100\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.1082 - sparse_categorical_accuracy: 0.3686\n",
      "Epoch 92/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1076 - sparse_categorical_accuracy: 0.3793\n",
      "Epoch 93/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1123 - sparse_categorical_accuracy: 0.3715\n",
      "Epoch 94/100\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.1065 - sparse_categorical_accuracy: 0.3753\n",
      "Epoch 95/100\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.1080 - sparse_categorical_accuracy: 0.3758\n",
      "Epoch 96/100\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.1084 - sparse_categorical_accuracy: 0.3788\n",
      "Epoch 97/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1078 - sparse_categorical_accuracy: 0.3799\n",
      "Epoch 98/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1081 - sparse_categorical_accuracy: 0.3759\n",
      "Epoch 99/100\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.1073 - sparse_categorical_accuracy: 0.3828\n",
      "Epoch 100/100\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1079 - sparse_categorical_accuracy: 0.3771\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, steps_per_epoch=train_steps_per_epoch, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "ebfd3237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: [130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 49, 43]\n",
      "actual   : [ 0  0  0  0  0  0  0  0  0  0  0  0  0 41 43]\n"
     ]
    }
   ],
   "source": [
    "index = 500\n",
    "\n",
    "test_feat_dict = {'training_sequence': train_feat_dict['training_sequence'][index:index+1],\n",
    "                'recency': train_feat_dict['recency'][index:index+1]}\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_feat_dict)).cache()\n",
    "\n",
    "y_pred = model.predict(test_dataset)\n",
    "output = []\n",
    "for i in range(15):\n",
    "    maxElement = np.amax(y_pred[i][0])\n",
    "    result = np.where(y_pred[i][0] == np.amax(y_pred[i][0]))\n",
    "    output.append(result[0][0])\n",
    "\n",
    "print(f'predicted: {output}')\n",
    "print(f'actual   : {train_target_tensor[index:index+1][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "8113a576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8958cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
